# Training configuration from Section 3.3
training:
  # Basic Training Parameters
  batch_size: 32
  learning_rate: 1.0e-4
  num_epochs: 100
  early_stopping_patience: 10
  max_grad_norm: 1.0

  # Optimizer Settings
  optimizer:
    name: "AdamW"
    betas: [0.9, 0.999]
    weight_decay: 0.01
    eps: 1.0e-8

  # Learning Rate Schedule
  lr_scheduler:
    name: "cosine"
    warmup_steps: 1000
    min_lr: 1.0e-6

  # Augmentation
  augmentation:
    enabled: true
    rotation_range: 5.0
    scale_range: [0.95, 1.05]
    contrast_range: [0.8, 1.2]
    noise_probability: 0.3

  # Mixed Precision Training
  amp:
    enabled: true
    opt_level: "O1"

  # Distributed Training
  distributed:
    enabled: false
    backend: "nccl"
    world_size: 1
    rank: 0

  # Checkpointing
  checkpoint:
    save_freq: 1000
    keep_last: 5
    save_best: true
    metric: "val_loss"

  # Logging
  logging:
    level: "INFO"
    log_freq: 100
    eval_freq: 1000

# Dataset Configuration
data:
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  max_tokens: 512
  min_freq: 5

  # Language-Specific Settings
  language_weights:
    Spanish-Nahuatl: 0.6
    Spanish-Mixtec: 0.25
    Spanish-Zapotec: 0.15

# Cultural Ontology
cultural_ontology:
  kinship:
    - "tlatoani"
    - "pilli"
    - "tecuhtli"
  cosmology:
    - "tonalli"
    - "teyolia"
    - "ihiyotl"
  ecological:
    - "milpa"
    - "altepetl"
    - "tepetl"
  ritual:
    - "tlamanalistli"
    - "nextlaualli"
    - "tequitl"
